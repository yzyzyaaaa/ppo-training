智能体落地“最后一公里”初探之Cursor在线强化学习
================================================================================

最后更新：10/20/2025。

相关阅读：:doc:`代码补全在线强化学习 <../examples/coding_online_rl>`。

1. 背景
------------------------------

最近，Cursor 团队推出了一种基于 **在线强化学习（Online RL）** 的新型 Tab 模型（ `链接 <https://mp.weixin.qq.com/s/ShalRibfp9YSE5UFS0GLVg>`_ ）。该模型将用户的每一次交互（如接受或拒绝建议）视作强化信号，直接参与模型的在线优化。在每日超过 4 亿次请求的真实反馈驱动下，模型能够以极高频率持续学习与改进，成为首个利用在线强化学习提升实际服务效果的成功案例

随着 **Agentic AI** 时代的到来，越来越多的智能体（Agent）被部署到真实生产环境中，并通过在线强化学习实现个性化与性能自适应优化。这一范式有望成为推动智能体“落地最后一公里”的关键技术路径。

嗅到这一趋势，我们团队基于大规模强化学习框架 **RLinf** 对 Cursor 的在线强化学习方案进行了复现实验，探索在线强化学习能否进一步提升模型的代码补全能力。具体而言，我们采用 **Continue** 作为代码编辑器端，用户可直接在 VSCode 中安装 `Continue 插件 <https://github.com/RLinf/continue>`_ 进行开发； **RLinf** 则作为后端系统，既负责大模型的 Serving，也承担在线强化学习的训练。值得注意的是，大模型的 Serving 后端也可以替换为已部署的 Agent 服务，而 **RLinf** 可灵活作为提供在线强化学习能力的通用组件接入系统。

2. 前情提要
------------------------------

2.1 代码补全功能简介
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

对于像 **Cursor** 这样的智能编程编辑器，核心功能之一就是高效、上下文感知的 **代码补全** 。在典型的代码补全任务中，当光标停留在某个位置时，编程助手会根据上下文给出插入内容的建议。实现这一能力的常见方式是 **FIM（Fill-In-the-Middle）** 任务：模型接收光标位置的上文和下文，预测中间应填充的内容。目前，大多数大语言模型（LLM）在预训练阶段已针对 FIM 任务进行过优化，只需按照以下格式组织输入：
`<|fim_prefix|>上文<|fim_suffix|>下文<|fim_middle|>`

模型即可生成合理的中间补全内容。这种设计使得 LLM 能够自然地适应编辑器中的代码补全场景。

2.2 Continue 插件的代码补全逻辑
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

在我们的实验中，我们基于开源 AI 编程IDE **Continue** 实现了完整的在线强化学习（Online RL）代码补全案例。

Continue 默认支持基于 FIM 格式的补全任务，但原生版本并不具备人类反馈（Human Feedback）上报功能。

为此，我们对插件进行了轻量级改造：

- 当用户按下 **Tab** 键接受补全建议时，插件上报 accepted=True；
- 当用户在 10 秒内未按下 Tab 或进行了其他编辑操作时，上报 accepted=False。

通过这种方式，我们能够直接从真实用户交互中获得强化信号（reward），无需额外训练一个 reward 模型去拟合人类偏好，实现了 **从人类到模型的直接强化学习闭环**。

3. 系统搭建
--------------------

3.1 流程概览
~~~~~~~~~~~~~~~~~~~~~~

整个在线强化学习的流程可以概括为三个核心步骤：

1. **交互与反馈采集**：

   Coding Agent 向用户提供代码补全建议，用户的“接受”或“拒绝”操作构成明确的强化信号。
2. **即时在线更新**：

   用户反馈传到 RLinf 的后端系统用于训练该生成模型，模型基于 On-policy 策略实时更新，并同步回线上 Agent，以获取新的交互反馈。
3. **效果验证与部署**：

   在线训练结束后，通过 **A/B** 测试 评估新模型的接受率是否优于原模型；若效果提升，则统一部署至线上环境。

3.1 RLinf-Online 搭建
~~~~~~~~~~~~~~~~~~~~~

下面介绍如何基于RLinf快速搭建该流程：

（1）RLinf Worker抽象

RLinf 框架提供了 Worker 的编程接口，这是 RLinf 构建整个框架的基本组件。Worker 表示一个可执行的组件，大的组件可以是推理实例、训练框架，小的组件可以是数据加载器等。通过继承 Worker 类，可以将一个具体的执行组件进行抽象，并提供和其他 Worker 交互、以及被 RLinf 调度、分配和管理的能力。

（2）RLinf Channel通信

RLinf 框架提供了高性能、易用的异步通信抽象 Channel，自适应使用优化过的点对点后端（如 CUDA IPC 和 NCCL），并封装为生产者-消费者队列的通信模式。因此 Worker1 -> Worker2 的通信可以如下实现：

.. code-block:: python

   self.comm_channel = Channel.create("Comm") #创建一个 channel

   Handle1 = self.worker1.rollout(
       output_channel=self.comm_channel,
   ) # 执行数据生成

   Handle2 = self.worker2.run_inference(
       input_channel=self.comm_channel,
   )# 执行inference流程

仅需3行代码即可实现 Worker1 -> Worker2 的通信逻辑，大大简化了代码逻辑。

（3）基于RLinf构建在线强化学习训练流程

有了Worker和Channel这两个基本元素，我们便可以搭建在线强化学习的整套训练流程。整体系统架构如下图所示。

.. raw:: html

   <img src="https://github.com/RLinf/misc/raw/main/pic/coding_online_rl_arch.png" width="800"/>

假设代码补全 Agent 已经作为一个完整的在线服务部署，由 **用户前端（User Frontend）** 与 **服务后端（Service Backend）** 组成。为了让这一线上系统具备在线强化学习（Online RL）能力，我们在其 **plugin** 层 引入了一个独立组件 —— **RLinf Runner** 。与长期运行的后台服务不同，RLinf Runner 并不是一个常驻进程，而是一个可以由线上系统的 **Controller** 按需调用的轻量级模块。我们为 RLinf Runner 设计了与线上 Agent 的交互接口，用于：

1. 获取在线数据，包括请求（request）、响应（response）内容以及用户交互反馈（accept/reject）；
2. 接收并更新模型权重，从而实现 Agent 策略的实时优化。

在 RLinf Runner 内部，我们将整个强化学习过程分解为三个核心 Worker：

- **Data Receiver**：负责接收并缓存来自线上系统的交互数据；
- **Compute Reward**：根据用户反馈计算即时奖励信号；
- **PPO Loss + Actor Trainer**：执行策略优化与模型更新。

这些 Worker 之间的通信通过 RLinf Channel 实现，它提供高性能、异步的数据传递机制，使整个在线训练流程能够以流式方式持续进行。当 Service Backend 的 Controller 启动 RLinf Runner 后，在线强化学习过程便会自动运行：系统从线上服务中接收数据、计算奖励、更新策略模型，并将改进后的模型权重实时回传至服务后端。为保障线上服务的稳定性，在线强化学习可首先在部分愿意参与新模型试验的用户群体中进行部署与验证。

4. 算法设计
--------------------

除了模块化的系统设计之外，我们也在 **在线强化学习（Online RL）算法设计** 上进行了深入探索。在 Online RL 场景中，每个请求（request）通常只对应一次响应与一次用户反馈（accept/reject），因此 **GRPO** 不再适用，因为它依赖于对同一输入的多样化响应组来计算相对偏好。为此，我们采用了改进后的 **PPO 算法** ，主要改动包括：去掉 **critic 模型** ，优势估计（advantage estimation）退化为 **蒙特卡洛回报（Monte Carlo return）** 。虽然这种方法可能带来较大的训练方差，但依靠 PPO 的 **clip 机制** 可以有效限制策略更新幅度，防止训练崩溃，从而实现一种 **高效且稳定的简化策略**。在代码补全的 Online RL 训练过程中， **reward 来源于用户反馈信号** （即用户的接受或拒绝操作）。

由于目前缺乏足够规模的真实 Online 使用场景，我们采用 **LLM 模拟用户打分（LLM-as-a-Judge）** 的方式，对模型生成的补全结果进行评分。具体地，我们使用 LLM（DeepSeek-V3.1）对模型生成的补全结果进行 0–10 分打分，平均得分作为模型在测试集上的综合表现指标。

5. 性能一览
--------------------

5.1 训练配置
~~~~~~~~~~~~~~~~~~~~~~

**数据集构建**

我们选用 **code-fim-v2** 数据集，该数据集包含多种编程语言的代码补全样本。我们从中筛选出 Python 样本，并进一步过滤掉补全内容过短的样本，最终保留约 **4000 条高质量数据**。其中 **3000 条** 用于训练， **1000 条** 用于测试。每个样本包含上文（prefix）与下文（suffix）代码片段，模型需根据上下文生成中间补全内容。

**主要参数**

实验基模型为 **Qwen2.5-Coder-1.5B** 。由于未加入 KL regularization，过高学习率可能导致模型遗忘原有分布，因此我们选择较低学习率(2e-6)以保持稳定收敛。同时，采用bf16 训练精度，相较于 fp16 在训练早期的梯度范数更稳定。

此外，为了快速验证强化学习在该任务上的有效性，我们还采用了 **GRPO（group size = 8）** 进行离线训练对比实验，以评估不同训练范式下模型在代码补全任务上的性能变化。

5.2 实验结果
~~~~~~~~~~~~~~~~~~~~~~

如图1所示，可以看到通过在线强化学习，模型性能持续增长。测试集结果如表1所示，Qwen2.5-Coder-1.5B-RLinf在测试集上提升效果明显(4.532 -> 6.897)，涨幅超50%，甚至超过同系列32B模型。这表明通过在线强化学习可以有效提升模型部署性能，并且小模型具有巨大潜力。

.. list-table::
   :widths: 50 50
   :header-rows: 0
   :align: center

   * - .. image:: https://github.com/qurakchin/misc/raw/docs/coding_rl_offline_reward/pic/coding_online_rl_offline_rewards.png
          :width: 100%
     - .. list-table::
          :header-rows: 1
          :align: center

          * - 模型
            - 分数
          * - Qwen2.5-Coder-1.5B
            - 4.532
          * - Qwen2.5-Coder-3B
            - 5.139
          * - Qwen2.5-Coder-7B
            - 5.68
          * - Qwen2.5-Coder-14B
            - 6.351
          * - Qwen2.5-Coder-32B
            - 6.545
          * - Qwen2.5-Coder-1.5B-RL
            - 6.897 (+52%)
   * - 图1 训练reward变化图
     - 表1 测试集得分（0-10 分）

6. 未来展望
--------------------

RLinf-online是团队在智能体在线优化方案的初步探索，当前版本仅采用人类代理的形式进行性能模拟，但其结果已经表明在线强化学习的无限潜力。团队正将该流程上线生产环境，在实际业务中进行测试。同时，RLinf团队期待与大家合作，共同探索大模型时代下的强化学习边界！
