# PickCube PPO Training

![Success Rate](https://img.shields.io/badge/Success%20Rate-99.7%25-brightgreen)
![Training Time](https://img.shields.io/badge/Training%20Time-3h45m-blue)
![Model Size](https://img.shields.io/badge/Params-287K-orange)
![Python](https://img.shields.io/badge/Python-3.10-blue)

ä½¿ç”¨PPO (Proximal Policy Optimization) ç®—æ³•è®­ç»ƒManiSkill PickCubeæœºå™¨äººæŠ“å–ä»»åŠ¡ï¼Œè¾¾åˆ°99.7%çš„æˆåŠŸç‡ã€‚

## ğŸ¥ æ¼”ç¤ºè§†é¢‘

è®­ç»ƒå¥½çš„æ¨¡å‹æ¼”ç¤ºï¼ˆ5ä¸ªepisodeså…¨éƒ¨æˆåŠŸï¼‰ï¼š
- æˆåŠŸç‡: 100% (5/5)
- å¹³å‡æ­¥æ•°: 18.4æ­¥
- è§†é¢‘æ—¶é•¿: 8.6ç§’

> æ³¨ï¼šæ¼”ç¤ºè§†é¢‘æ–‡ä»¶è¾ƒå¤§ï¼ˆ1.2MBï¼‰ï¼Œæœªä¸Šä¼ åˆ°GitHubã€‚è¿è¡Œé¡¹ç›®åä¼šåœ¨`recordings/1.mp4`ç”Ÿæˆã€‚

## âš¡ å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹
```bash
bash run_complete_pipeline.sh
```

è¿™å°†è‡ªåŠ¨å®Œæˆï¼šè®­ç»ƒ â†’ æ¨¡å‹æå– â†’ è§†é¢‘ç”Ÿæˆ

### åˆ†æ­¥æ‰§è¡Œ
```bash
# Step 1: è®­ç»ƒæ¨¡å‹ (2000 epochs, ~3.7å°æ—¶)
bash run_ppo_maniskill.sh

# Step 2: æå–æ¨¡å‹ (~30ç§’)
python extract_with_dcp.py

# Step 3: ç”Ÿæˆæ¼”ç¤ºè§†é¢‘ (~1åˆ†é’Ÿ)
python demo_with_trained_model.py
```

## ğŸ› ï¸ ç¯å¢ƒè®¾ç½®

### å‰ç½®è¦æ±‚
- Python 3.10+
- CUDA-capable GPU
- Conda

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/YOUR_USERNAME/pickcube-ppo-training.git
cd pickcube-ppo-training

# 2. åˆ›å»ºCondaç¯å¢ƒ
conda create -n sapien_yzy python=3.10
conda activate sapien_yzy

# 3. å®‰è£…RLinfæ¡†æ¶
git clone https://github.com/garrett4wade/RLinf.git
cd RLinf && pip install -e .
cd ..

# 4. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 5. è¿è¡Œè®­ç»ƒ
bash run_ppo_maniskill.sh
```

## ğŸ“Š è®­ç»ƒç»“æœ

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | åˆå§‹å€¼ | æœ€ç»ˆå€¼ | æå‡ |
|------|--------|--------|------|
| **æˆåŠŸç‡** | 0% | **99.7%** | +99.7% |
| **å¹³å‡æ­¥æ•°** | 50æ­¥ | **18.6æ­¥** | æ•ˆç‡â†‘63% |
| **å¹³å‡å¥–åŠ±** | 0.05 | **0.392** | +684% |
| **è§£é‡Šæ–¹å·®** | - | **85.1%** | ä»·å€¼ç½‘ç»œä¼˜ç§€ |

### å­¦ä¹ æ›²çº¿

```
Epoch    0: æˆåŠŸç‡ =  0%,   æ­¥æ•° = 50
Epoch  500: æˆåŠŸç‡ = 50%,  æ­¥æ•° = 35
Epoch 1000: æˆåŠŸç‡ = 85%,  æ­¥æ•° = 25
Epoch 1500: æˆåŠŸç‡ = 95%,  æ­¥æ•° = 22
Epoch 2000: æˆåŠŸç‡ = 99.7%, æ­¥æ•° = 18.6  âœ“
```

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ maniskill_ppo_test.yaml       # è®­ç»ƒé…ç½®
â”œâ”€â”€ run_ppo_maniskill.sh          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ clean_output.py               # è¾“å‡ºè¿‡æ»¤
â”œâ”€â”€ extract_with_dcp.py           # æ¨¡å‹æå–
â”œâ”€â”€ demo_with_trained_model.py    # è§†é¢‘ç”Ÿæˆ
â”œâ”€â”€ run_complete_pipeline.sh      # ä¸€é”®è¿è¡Œ
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ README.md                     # æœ¬æ–‡ä»¶
â”œâ”€â”€ PROJECT_FILES.txt             # è¯¦ç»†æ–‡ä»¶æ¸…å•
â””â”€â”€ recordings/                   # è¾“å‡ºç›®å½•ï¼ˆæœ¬åœ°ç”Ÿæˆï¼‰
    â”œâ”€â”€ 1.mp4                     # æ¼”ç¤ºè§†é¢‘
    â”œâ”€â”€ trained_model.pth         # è®­ç»ƒå¥½çš„æ¨¡å‹
    â”œâ”€â”€ training_log.txt          # è®­ç»ƒæ—¥å¿—
    â””â”€â”€ rlinf_logs/               # TensorBoardæ•°æ®
```

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹æ¶æ„
- **ç±»å‹**: MLP Policy
- **è¾“å…¥**: 42ç»´çŠ¶æ€ï¼ˆæœºå™¨äººå…³èŠ‚ä½ç½®ã€æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ã€ç‰©ä½“ä½ç½®ç­‰ï¼‰
- **è¾“å‡º**: 8ç»´åŠ¨ä½œï¼ˆ7ä¸ªå…³èŠ‚ + 1ä¸ªå¤¹çˆªï¼‰
- **éšè—å±‚**: 3å±‚ Ã— 256ç¥ç»å…ƒ
- **å‚æ•°é‡**: 287,504

### è®­ç»ƒé…ç½®
- **Epochs**: 2000
- **å¹¶è¡Œç¯å¢ƒ**: 128ä¸ª
- **æ‰¹æ¬¡å¤§å°**: 640
- **å­¦ä¹ ç‡**: 3e-4
- **æŠ˜æ‰£å› å­**: Î³=0.99
- **GAE Lambda**: 0.95
- **PPOè£å‰ª**: Îµ=0.2

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œåœ¨`recordings/`ç›®å½•ä¸‹ç”Ÿæˆï¼š

- `1.mp4` - æ¼”ç¤ºè§†é¢‘
- `trained_model.pth` - PyTorchæ ‡å‡†æ ¼å¼æ¨¡å‹
- `training_log.txt` - å®Œæ•´è®­ç»ƒæ—¥å¿—
- `rlinf_logs/tensorboard/` - TensorBoardå¯è§†åŒ–æ•°æ®
- `rlinf_logs/pickcube_mlp/checkpoints/` - æ¨¡å‹æ£€æŸ¥ç‚¹

## ğŸ“ˆ æŸ¥çœ‹è®­ç»ƒæ›²çº¿

```bash
tensorboard --logdir=recordings/rlinf_logs/tensorboard
```

è®¿é—® http://localhost:6006 æŸ¥çœ‹è®­ç»ƒæ›²çº¿ã€‚

## ğŸ¯ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

```python
import torch
from rlinf.models.embodiment.mlp_policy import MLPPolicy

# åŠ è½½æ¨¡å‹
checkpoint = torch.load('recordings/trained_model.pth')
model = MLPPolicy(**checkpoint['model_config'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# æ¨ç†
action = model.actor_mean(state_tensor)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ License

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

- [RLinf](https://github.com/garrett4wade/RLinf) - å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶
- [ManiSkill](https://github.com/haosulab/ManiSkill) - æœºå™¨äººä»¿çœŸç¯å¢ƒ
- [SAPIEN](https://sapien.ucsd.edu/) - ç‰©ç†ä»¿çœŸå¼•æ“

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤Issueæˆ–è”ç³» [your-email@example.com]

---

**Star â­ è¿™ä¸ªé¡¹ç›®å¦‚æœå®ƒå¯¹ä½ æœ‰å¸®åŠ©ï¼**
